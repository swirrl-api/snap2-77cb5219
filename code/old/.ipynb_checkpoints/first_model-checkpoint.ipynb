{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba88d58b-3f94-494c-a1fa-de51a2fdb13b",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b58ab7-6891-4f0b-b5d1-99b177fa93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ebb57-9037-4cda-9c25-afd372e4d544",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828fdb6-4193-4085-9459-cb09d686333c",
   "metadata": {},
   "source": [
    "### BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f7c25e0d-0203-4542-afdb-7ee976f60702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['lon', 'lat', 'time', 'coefficient', 'spatial_ref'])\n"
     ]
    }
   ],
   "source": [
    "file_path = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_BC_1990-06-01_1990-07-01.nc'\n",
    "data = nc.Dataset(file_path)\n",
    "print(data.variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46b79d37-7e1b-477c-9577-e4384ef9d5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['lon', 'lat', 'time', 'coefficient', 'spatial_ref'])\n"
     ]
    }
   ],
   "source": [
    "file_path = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_BC_1990-07-01_1990-08-01.nc'\n",
    "data = nc.Dataset(file_path)\n",
    "print(data.variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bd650b1a-5605-494c-8eab-0adfca855d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['lon', 'lat', 'time', 'coefficient', 'spatial_ref'])\n"
     ]
    }
   ],
   "source": [
    "file_path = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_BC_1990-08-01_1990-09-01.nc'\n",
    "data = nc.Dataset(file_path)\n",
    "print(data.variables.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed18bd-fc2a-47f6-aa68-e5bea374117b",
   "metadata": {},
   "source": [
    "## Merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92ef45-2e9c-48b0-9feb-eea3f112e65d",
   "metadata": {},
   "source": [
    "### BC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e310a-4b3d-4f4f-bba7-5c38038881d2",
   "metadata": {},
   "source": [
    "#### On y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16191370-d434-499f-8be3-3b3d78e81ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_BC_1990-07-01_1990-08-01.nc', '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_BC_1990-06-01_1990-07-01.nc', '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_BC_1990-08-01_1990-09-01.nc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_pattern = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_BC_1990-*.nc'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "print(file_paths)\n",
    "type(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "145c0ebc-fa94-406d-a433-bb7e249b42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_BC_1990-*.nc'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "# Open and concatenate files along a dimension (e.g., time)\n",
    "bc_1990 = xr.open_mfdataset(file_paths)\n",
    "\n",
    "# Rename coefficient\n",
    "bc_1990 = bc_1990.rename({'coefficient': 'BC'})\n",
    "\n",
    "# Save the merged data to a new NetCDF file\n",
    "bc_1990.to_netcdf(\"bc_1990.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d1cdce9a-9b55-4df7-980b-891206d49ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 14MB\n",
      "Dimensions:      (lon: 268, lat: 141, time: 95)\n",
      "Coordinates:\n",
      "  * lon          (lon) float32 1kB -26.0 -25.75 -25.5 -25.25 ... 40.5 40.75 41.0\n",
      "  * lat          (lat) float32 564B 35.8 36.05 36.3 36.55 ... 70.5 70.75 71.0\n",
      "  * time         (time) datetime64[ns] 760B 1990-06-01 1990-06-02 ... 1990-09-01\n",
      "Data variables:\n",
      "    BC           (time, lat, lon) float32 14MB dask.array<chunksize=(31, 141, 268), meta=np.ndarray>\n",
      "    spatial_ref  (time) int32 380B -2147483647 -2147483647 ... -2147483647\n"
     ]
    }
   ],
   "source": [
    "print(bc_1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ed3d8b6-b5ba-4d19-aa4f-02d621935dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bc_1990.to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "670ba38d-fdb3-4dd8-9f03-fc264e5220db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          lon        lat       time  coefficient  spatial_ref\n",
      "0       -26.0  35.799999 1990-06-01          0.0  -2147483647\n",
      "1       -26.0  35.799999 1990-06-02          0.0  -2147483647\n",
      "2       -26.0  35.799999 1990-06-03          0.0  -2147483647\n",
      "3       -26.0  35.799999 1990-06-04          0.0  -2147483647\n",
      "4       -26.0  35.799999 1990-06-05          0.0  -2147483647\n",
      "...       ...        ...        ...          ...          ...\n",
      "3589855  41.0  71.000000 1990-08-28          0.0  -2147483647\n",
      "3589856  41.0  71.000000 1990-08-29          0.0  -2147483647\n",
      "3589857  41.0  71.000000 1990-08-30          0.0  -2147483647\n",
      "3589858  41.0  71.000000 1990-08-31          0.0  -2147483647\n",
      "3589859  41.0  71.000000 1990-09-01          0.0  -2147483647\n",
      "\n",
      "[3589860 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4eace-db83-4c2e-b0c3-859db1a51a2e",
   "metadata": {},
   "source": [
    "### CC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93571621-1ad5-4e3c-b16c-614480095e59",
   "metadata": {},
   "source": [
    "#### y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aee1412f-f210-412e-9fb9-d6ba47ffef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_CC_1990-08-01_1990-09-01.nc', '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_CC_1990-06-01_1990-07-01.nc', '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_CC_1990-07-01_1990-08-01.nc']\n"
     ]
    }
   ],
   "source": [
    "file_pattern = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_CC_1990-*.nc'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "de2f4b19-3161-4361-b86f-cdab54515bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_CC_1990-*.nc'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "# Open and concatenate files\n",
    "cc_1990 = xr.open_mfdataset(file_paths)\n",
    "\n",
    "# Rename coefficient\n",
    "cc_1990 = cc_1990.rename({'coefficient': 'CC'})\n",
    "\n",
    "# Save the merged data to a new NetCDF file\n",
    "cc_1990.to_netcdf(\"cc_1990.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6c368-0cda-44eb-ba53-9d731eb45d4e",
   "metadata": {},
   "source": [
    "### DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aaac600f-b7bc-4da4-9f06-f4e042bb7737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_DC_1990-08-01_1990-09-01.nc', '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_DC_1990-06-01_1990-07-01.nc', '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_DC_1990-07-01_1990-08-01.nc']\n"
     ]
    }
   ],
   "source": [
    "file_pattern = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_DC_1990-*.nc'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6962bab2-96ee-4a07-a4f6-10169ab301a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_CC_1990-*.nc'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "# Open and concatenate files\n",
    "dc_1990 = xr.open_mfdataset(file_paths)\n",
    "\n",
    "# Rename coefficient\n",
    "dc_1990 = dc_1990.rename({'coefficient': 'DC'})\n",
    "\n",
    "# Save the merged data to a new NetCDF file\n",
    "dc_1990.to_netcdf(\"dc_1990.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60a19d-a596-4d3d-81ba-fb6b06cd6834",
   "metadata": {},
   "source": [
    "### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7d29945-696e-4ae9-b4c6-05a46c8f5080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_ID_1990-07-01_1990-08-01.nc', '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_ID_1990-06-01_1990-07-01.nc', '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_ID_1990-08-01_1990-09-01.nc']\n"
     ]
    }
   ],
   "source": [
    "file_pattern = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_ID_1990-*.nc'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6bc9d8e1-2fc7-4f00-b6fd-7d745809f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_CC_1990-*.nc'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "# Open and concatenate files\n",
    "id_1990 = xr.open_mfdataset(file_paths)\n",
    "\n",
    "# Rename coefficient\n",
    "id_1990 = id_1990.rename({'coefficient': 'ID'})\n",
    "\n",
    "# Save the merged data to a new NetCDF file\n",
    "id_1990.to_netcdf(\"id_1990.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ff0d8-a944-4dbe-acb3-2d695d6a4e0e",
   "metadata": {},
   "source": [
    "### OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4c1ac9b-3688-4079-84d3-61c085913a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_OD_1990-06-01_1990-07-01.nc', '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_OD_1990-08-01_1990-09-01.nc', '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_OD_1990-07-01_1990-08-01.nc']\n"
     ]
    }
   ],
   "source": [
    "file_pattern = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_OD_1990-*.nc'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "920dd7d9-4ea2-4e17-815b-53e454ad78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = '../private/complex_network_coefficients/1990-1999_run_20240122_1017/rasterfiles/Europe/1990/CN_Europe_0.25x0.25deg_CC_1990-*.nc'\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "# Open and concatenate files\n",
    "od_1990 = xr.open_mfdataset(file_paths)\n",
    "\n",
    "# Rename coefficient\n",
    "od_1990 = od_1990.rename({'coefficient': 'OD'})\n",
    "\n",
    "# Save the merged data to a new NetCDF file\n",
    "od_1990.to_netcdf(\"od_1990.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1a13f8dc-913e-4faf-96a9-3d24b4a282fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 14MB\n",
      "Dimensions:      (lon: 268, lat: 141, time: 95)\n",
      "Coordinates:\n",
      "  * lon          (lon) float32 1kB -26.0 -25.75 -25.5 -25.25 ... 40.5 40.75 41.0\n",
      "  * lat          (lat) float32 564B 35.8 36.05 36.3 36.55 ... 70.5 70.75 71.0\n",
      "  * time         (time) datetime64[ns] 760B 1990-06-01 1990-06-02 ... 1990-09-01\n",
      "Data variables:\n",
      "    OD           (time, lat, lon) float32 14MB dask.array<chunksize=(31, 141, 268), meta=np.ndarray>\n",
      "    spatial_ref  (time) int32 380B -2147483647 -2147483647 ... -2147483647\n"
     ]
    }
   ],
   "source": [
    "print(od_1990)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce366ccf-b6d8-4173-976c-5ed2167cf86d",
   "metadata": {},
   "source": [
    "### On x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0dbdc862-ba8b-4c3b-a251-0a49cb59404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 72MB\n",
      "Dimensions:      (lon: 268, lat: 141, time: 95)\n",
      "Coordinates:\n",
      "  * lon          (lon) float32 1kB -26.0 -25.75 -25.5 -25.25 ... 40.5 40.75 41.0\n",
      "  * lat          (lat) float32 564B 35.8 36.05 36.3 36.55 ... 70.5 70.75 71.0\n",
      "  * time         (time) datetime64[ns] 760B 1990-06-01 1990-06-02 ... 1990-09-01\n",
      "Data variables:\n",
      "    BC           (time, lat, lon) float32 14MB ...\n",
      "    spatial_ref  (time) int32 380B -2147483647 -2147483647 ... -2147483647\n",
      "    CC           (time, lat, lon) float32 14MB ...\n",
      "    DC           (time, lat, lon) float32 14MB ...\n",
      "    ID           (time, lat, lon) float32 14MB ...\n",
      "    OD           (time, lat, lon) float32 14MB ...\n"
     ]
    }
   ],
   "source": [
    "# Define filenames\n",
    "bc = \"bc_1990.nc\"\n",
    "cc = \"cc_1990.nc\"\n",
    "dc = \"dc_1990.nc\"\n",
    "id = \"id_1990.nc\"\n",
    "od = \"od_1990.nc\"\n",
    "\n",
    "# Open datasets\n",
    "ds1 = xr.open_dataset(bc)\n",
    "ds2 = xr.open_dataset(cc)\n",
    "ds3 = xr.open_dataset(dc)\n",
    "ds4 = xr.open_dataset(id)\n",
    "ds5 = xr.open_dataset(od)\n",
    "\n",
    "# Merge datasets\n",
    "merged_1990 = xr.combine_by_coords([ds1, ds2, ds3, ds4, ds5])\n",
    "\n",
    "print(merged_1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1f546b7e-cf89-4ac9-b933-49fa982fb55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged data to a new NetCDF file\n",
    "merged_1990.to_netcdf(\"merged_1990.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6ec7cfd9-55ce-43bf-a8b7-f691ba58abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'lon': 268, 'lat': 141, 'time': 95})\n"
     ]
    }
   ],
   "source": [
    "print(merged_1990.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d199f7f7-831f-4261-ae71-37fc29c9e8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 6B\n",
      "Dimensions:      ()\n",
      "Data variables:\n",
      "    BC           bool 1B True\n",
      "    spatial_ref  bool 1B False\n",
      "    CC           bool 1B True\n",
      "    DC           bool 1B True\n",
      "    ID           bool 1B True\n",
      "    OD           bool 1B True\n"
     ]
    }
   ],
   "source": [
    "contains_na = merged_1990.isnull().any()\n",
    "print(contains_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d716bdea-629e-49fe-805f-c26e83c35bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 48B\n",
      "Dimensions:      ()\n",
      "Data variables:\n",
      "    BC           int64 8B 5215\n",
      "    spatial_ref  int64 8B 0\n",
      "    CC           int64 8B 5513\n",
      "    DC           int64 8B 5513\n",
      "    ID           int64 8B 5513\n",
      "    OD           int64 8B 5513\n"
     ]
    }
   ],
   "source": [
    "missing_values = merged_1990.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb8ddf-b1f4-4e2e-b2b0-6cb38c075a02",
   "metadata": {},
   "source": [
    "## Check shape, dimensions, and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4487dcaa-7b3b-4450-a094-8c81ca147326",
   "metadata": {},
   "outputs": [],
   "source": [
    "with merged_1990 as nc:\n",
    "    # Assuming variable names are lon, lat, time, coef, and srs\n",
    "    lon = nc['lon'][:]\n",
    "    lat = nc['lat'][:]\n",
    "    time = nc['time'][:]\n",
    "    bc = nc['BC'][:]\n",
    "    cc = nc['CC'][:]\n",
    "    dc = nc['DC'][:]\n",
    "    id = nc['ID'][:]\n",
    "    od = nc['OD'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9bb11176-20af-4956-8d85-5933de129eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(268,) (141,) (95,) (95, 141, 268) (95, 141, 268) (95, 141, 268) (95, 141, 268) (95, 141, 268)\n"
     ]
    }
   ],
   "source": [
    "print(lon.shape ,lat.shape, time.shape, bc.shape, cc.shape, dc.shape, id.shape, od.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6849ca7b-0e39-423e-bb94-9034c17f8901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 3 3 3 3 3\n"
     ]
    }
   ],
   "source": [
    "print(lon.ndim ,lat.ndim, time.ndim, bc.ndim, cc.ndim, dc.ndim, id.ndim, od.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "42494ac8-9675-42d2-bb36-b50df20786c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268 141 95 3589860 3589860 3589860 3589860 3589860\n"
     ]
    }
   ],
   "source": [
    "print(lon.size ,lat.size, time.size, bc.size, cc.size, dc.size, id.size, od.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed15efe7-78b7-40f6-a10c-b6b35346cf8e",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "73c483a0-2377-417c-a913-71e5eaf32f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nc_data(filepath):\n",
    "  with Dataset(filepath) as nc:\n",
    "    # Assuming variable names are lon, lat, time, coef, and srs\n",
    "    BC = nc['BC'][:]\n",
    "    CC = nc['CC'][:]\n",
    "    DC = nc['DC'][:]\n",
    "    ID = nc['ID'][:]\n",
    "    OD = nc['OD'][:]\n",
    "\n",
    "    # Reshape coefficient (assuming features are lat*lon values)\n",
    "    # coef = coef.reshape(-1) \n",
    "\n",
    "    # Reshape coefficient to 2D for LSTM (timesteps, features) - assuming features are lat*lon values\n",
    "    BC = BC.reshape((time.size, -1))\n",
    "    CC = CC.reshape((time.size, -1))\n",
    "    DC = DC.reshape((time.size, -1))\n",
    "    ID = ID.reshape((time.size, -1))\n",
    "    OD = OD.reshape((time.size, -1))\n",
    "      \n",
    "    data = np.stack([BC, CC, DC, ID, OD], axis=-1)  # Combine lon, lat, time, and features\n",
    "    return data\n",
    "\n",
    "test = load_nc_data(\"merged_1990.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4cdb0642-d71a-4102-86fb-fdda78614b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486011e-0360-428f-a9be-92acf74508c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters (adjust as needed)\n",
    "timesteps = 7  # Number of past days to use for prediction\n",
    "n_features = lon.size * lat.size  # Assuming features are lat*lon values (modify if different)\n",
    "n_hidden = 64  # Number of units in the LSTM layer\n",
    "\n",
    "# Load data from multiple .nc files (replace with your logic)\n",
    "data_list = []\n",
    "for filename in ['file1.nc', 'file2.nc', 'file3.nc']:  # Replace with your filenames\n",
    "  data_list.append(load_nc_data(filename))\n",
    "\n",
    "# Stack data from multiple files and create sequences\n",
    "data = np.stack(data_list)\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(data) - timesteps):\n",
    "  X.append(data[i:i+timesteps, :, :])  # Select past days as features\n",
    "  y.append(data[i+timesteps, :, :])  # Prediction target is next day's data\n",
    "  \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Handle spatial reference (optional):\n",
    "# If your .nc files use different spatial references, you might need preprocessing\n",
    "# to ensure consistent coordinates. This could involve reprojection or using \n",
    "# the reference information for interpretation.\n",
    "\n",
    "# Define LSTM model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(n_hidden, activation='relu', input_shape=(timesteps, n_features)))\n",
    "model.add(keras.layers.Dense(n_features))  # Output layer with same number of features\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model (replace with your training data)\n",
    "model.fit(X[:int(len(X)*0.8)], y[:int(len(y)*0.8)], epochs=100, batch_size=32)  # Train on 80% of data\n",
    "\n",
    "# Make predictions on a new sequence (replace with your new data)\n",
    "new_data = load_nc_data('new_file.nc')\n",
    "new_data = new_data[-timesteps:, :, :]  # Select the last timesteps for prediction\n",
    "new_data = new_data.reshape(1, timesteps, n_features)  # Reshape for prediction\n",
    "predicted_trajectory = model.predict(new_data)\n",
    "\n",
    "# Reshape predicted trajectory (optional - depends on coefficient structure)\n",
    "predicted_trajectory = predicted_trajectory.reshape(-1, lat.size, lon.size)  # Assuming output is lat*lon\n",
    "\n",
    "# Use the predicted trajectory for further analysis or visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c44ba-634f-4357-83ff-894ea648eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from netCDF4 import Dataset  # Assuming you use netCDF4 for reading .nc files\n",
    "\n",
    "# Function to load data from a specific .nc file\n",
    "def load_nc_data(filepath):\n",
    "  with Dataset(filepath) as nc:\n",
    "    # Assuming variable names are lon, lat, time, coef, and srs\n",
    "    lon = nc['lon'][:]\n",
    "    lat = nc['lat'][:]\n",
    "    time = nc['time'][:]\n",
    "    coef = nc['coef'][:]  # Assuming coefficient is a 3D array (time, lat, lon)\n",
    "    # Handle spatial reference (srs) - more on this later\n",
    "\n",
    "    # Reshape coefficient to 2D for LSTM (timesteps, features) - assuming features are lat*lon values\n",
    "    coef = coef.reshape((time.size, -1))\n",
    "    data = np.stack([lon, lat, time, coef], axis=-1)  # Combine lon, lat, time, and features\n",
    "    return data\n",
    "\n",
    "# Define hyperparameters (adjust as needed)\n",
    "timesteps = 7  # Number of past days to use for prediction\n",
    "n_features = lon.size * lat.size  # Assuming features are lat*lon values (modify if different)\n",
    "n_hidden = 64  # Number of units in the LSTM layer\n",
    "\n",
    "# Load data from multiple .nc files (replace with your logic)\n",
    "data_list = []\n",
    "for filename in ['file1.nc', 'file2.nc', 'file3.nc']:  # Replace with your filenames\n",
    "  data_list.append(load_nc_data(filename))\n",
    "\n",
    "# Stack data from multiple files and create sequences\n",
    "data = np.stack(data_list)\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(data) - timesteps):\n",
    "  X.append(data[i:i+timesteps, :, :])  # Select past days as features\n",
    "  y.append(data[i+timesteps, :, :])  # Prediction target is next day's data\n",
    "  \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Handle spatial reference (optional):\n",
    "# If your .nc files use different spatial references, you might need preprocessing\n",
    "# to ensure consistent coordinates. This could involve reprojection or using \n",
    "# the reference information for interpretation.\n",
    "\n",
    "# Define LSTM model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(n_hidden, activation='relu', input_shape=(timesteps, n_features)))\n",
    "model.add(keras.layers.Dense(n_features))  # Output layer with same number of features\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the model (replace with your training data)\n",
    "model.fit(X[:int(len(X)*0.8)], y[:int(len(y)*0.8)], epochs=100, batch_size=32)  # Train on 80% of data\n",
    "\n",
    "# Make predictions on a new sequence (replace with your new data)\n",
    "new_data = load_nc_data('new_file.nc')\n",
    "new_data = new_data[-timesteps:, :, :]  # Select the last timesteps for prediction\n",
    "new_data = new_data.reshape(1, timesteps, n_features)  # Reshape for prediction\n",
    "predicted_trajectory = model.predict(new_data)\n",
    "\n",
    "# Reshape predicted trajectory (optional - depends on coefficient structure)\n",
    "predicted_trajectory = predicted_trajectory.reshape(-1, lat.size, lon.size)  # Assuming output is lat*lon\n",
    "\n",
    "# Use the predicted trajectory for further analysis or visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7225772a-a315-46de-aeb1-3bd6574a7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_dataset(\"merged_1990.nc\")\n",
    "# Select desired variables (BC, CC, DC) and convert to NumPy arrays\n",
    "features = data[[\"BC\", \"CC\", \"DC\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e8801719-795a-4574-8641-2d731487d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Mapping.values of <xarray.Dataset> Size: 43MB\n",
      "Dimensions:  (time: 95, lat: 141, lon: 268)\n",
      "Coordinates:\n",
      "  * lon      (lon) float32 1kB -26.0 -25.75 -25.5 -25.25 ... 40.5 40.75 41.0\n",
      "  * lat      (lat) float32 564B 35.8 36.05 36.3 36.55 ... 70.25 70.5 70.75 71.0\n",
      "  * time     (time) datetime64[ns] 760B 1990-06-01 1990-06-02 ... 1990-09-01\n",
      "Data variables:\n",
      "    BC       (time, lat, lon) float32 14MB ...\n",
      "    CC       (time, lat, lon) float32 14MB ...\n",
      "    DC       (time, lat, lon) float32 14MB ...>\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "037719fe-12aa-4304-8e25-d3f4e8f2eeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2e34c87c-976b-4453-9b56-c1cfb6e7a935",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'shift'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[220], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Load data from NetCDF file\u001b[39;00m\n\u001b[1;32m     47\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_1990.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Define model parameters\u001b[39;00m\n\u001b[1;32m     51\u001b[0m n_timesteps, lon, lat, n_features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[220], line 24\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m features \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOD\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Optional preprocessing (e.g., normalization)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Define target labels (shifted features for prediction)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshift\u001b[49m(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shifted by one time step\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39mvalues, labels\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'shift'"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define functions for data loading and preprocessing\n",
    "def load_data(filename):\n",
    "  \"\"\"\n",
    "  Loads data from a NetCDF file.\n",
    "\n",
    "  Args:\n",
    "      filename (str): Path to the NetCDF file.\n",
    "\n",
    "  Returns:\n",
    "      tuple: (features, labels)\n",
    "          - features: A NumPy array of shape (time_steps, n_samples, lon, lat, n_features) containing the input features.\n",
    "          - labels: A NumPy array of shape (time_steps, n_samples, lon, lat, n_features) containing the target labels (shifted features).\n",
    "  \"\"\"\n",
    "  data = xr.open_dataset(filename)\n",
    "  # Select desired variables (BC, CC, DC) and convert to NumPy arrays\n",
    "  features = data[[\"BC\", \"CC\", \"DC\", \"ID\", \"OD\"]].values\n",
    "  # Optional preprocessing (e.g., normalization)\n",
    "  # ...\n",
    "  # Define target labels (shifted features for prediction)\n",
    "  labels = features.shift(x=1)  # Shifted by one time step\n",
    "  return features.values, labels.values\n",
    "\n",
    "# Define LSTM model\n",
    "def create_model(n_features, n_timesteps, n_hidden):\n",
    "  \"\"\"\n",
    "  Creates an LSTM model for trajectory prediction.\n",
    "\n",
    "  Args:\n",
    "      n_features (int): Number of input features (variables).\n",
    "      n_timesteps (int): Number of time steps in the input sequence.\n",
    "      n_hidden (int): Number of neurons in the LSTM layer.\n",
    "\n",
    "  Returns:\n",
    "      tf.keras.Model: The compiled LSTM model.\n",
    "  \"\"\"\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(n_hidden, activation='relu', input_shape=(n_timesteps, features.shape[2], features.shape[3], n_features)))\n",
    "  model.add(Dense(n_features * features.shape[2] * features.shape[3], activation='linear'))\n",
    "  model.compile(loss='mse', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "# Load data from NetCDF file\n",
    "filename = \"merged_1990.nc\"\n",
    "features, labels = load_data(filename)\n",
    "\n",
    "# Define model parameters\n",
    "n_timesteps, lon, lat, n_features = features.shape\n",
    "n_hidden = 64  # Adjust this based on your data complexity\n",
    "\n",
    "# Create and train the LSTM model\n",
    "model = create_model(n_features, n_timesteps, n_hidden)\n",
    "model.fit(features, labels, epochs=10, batch_size=32)  # Adjust epochs and batch_size as needed\n",
    "\n",
    "# Use the model for prediction\n",
    "# ... (code to prepare new data for prediction and use model.predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34b515-b2fa-466c-a51e-5e88ffc92b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load data from NetCDF file\n",
    "filename = \"your_data.nc\"\n",
    "data = xr.open_dataset(filename)\n",
    "\n",
    "# Select clustering coefficient and heatwave classification variables\n",
    "clustering_coefficient = data[\"clustering_coefficient\"].values\n",
    "heatwave_classification = data[\"heatwave_classification\"].values.flatten()  # Flatten for classification\n",
    "\n",
    "# Preprocess data (e.g., normalization, sequence creation)\n",
    "# ... (code to prepare data for LSTM)\n",
    "\n",
    "# Define model parameters (adjust based on data complexity)\n",
    "n_features = 1  # Assuming clustering coefficient is the only feature\n",
    "n_timesteps = ...  # Length of your sequence for LSTM\n",
    "n_hidden = 64\n",
    "\n",
    "# Define LSTM model (replace with your specific model architecture)\n",
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(n_hidden, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "  model.add(Dense(3, activation='softmax'))  # 3 classes (pre-heatwave, heatwave, post-heatwave)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "# Train the LSTM model (replace with your training code)\n",
    "model = create_model()\n",
    "model.fit(..., ...)  # Replace with training data and parameters\n",
    "\n",
    "# Use the trained model to classify new data points\n",
    "# ... (code to prepare and use the model for prediction)\n",
    "\n",
    "# Analyze predictions to identify heatwave transitions\n",
    "# Look for transitions between predicted classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a07a9-ee10-4cf6-838f-0aaf7221d807",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
